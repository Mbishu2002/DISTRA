{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8190b79",
      "metadata": {
        "id": "b8190b79"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "dca9bee4",
      "metadata": {
        "id": "dca9bee4"
      },
      "outputs": [],
      "source": [
        "flood_data = pd.read_csv(\"/content/datasets/kerala.csv\")\n",
        "df_cleaned = flood_data.dropna()\n",
        "\n",
        "# Save the cleaned DataFrame to a new CSV file\n",
        "df_cleaned.to_csv('/content/datasets/cleaned_data.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccdc43ac",
      "metadata": {
        "id": "ccdc43ac",
        "outputId": "6dc1c487-db8a-4a4f-c10e-52feb03fd939",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.40)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.35.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wandb\n",
        "\n",
        "# Paste your api key here\n",
        "os.environ[\"WANDB_API_KEY\"] = '5113c6e4fced8cbf6ed17e7bfd6402b76f3fc454'\n",
        "\n",
        "wandb.init()\n",
        "# Feel free to change these and experiment !!\n",
        "config = wandb.config\n",
        "config.learning_rate = 2e-5\n",
        "config.batch_size = 32\n",
        "config.output_size = 1\n",
        "config.input_size = 1\n",
        "config.hidden_size = 64\n",
        "config.epochs = 80"
      ],
      "metadata": {
        "id": "kLx5MvgeCdv8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "087ffae1-9862-4381-de50-be5006589761"
      },
      "id": "kLx5MvgeCdv8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfmbishu\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231116_083854-klpu8lux</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/fmbishu/uncategorized/runs/klpu8lux' target=\"_blank\">easy-glitter-1</a></strong> to <a href='https://wandb.ai/fmbishu/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/fmbishu/uncategorized' target=\"_blank\">https://wandb.ai/fmbishu/uncategorized</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/fmbishu/uncategorized/runs/klpu8lux' target=\"_blank\">https://wandb.ai/fmbishu/uncategorized/runs/klpu8lux</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4c0a2ece",
      "metadata": {
        "id": "4c0a2ece"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class FloodDataset(Dataset):\n",
        "    def __init__(self, data_file, transform=None):\n",
        "        self.df = pd.read_csv(data_file)\n",
        "\n",
        "        # Drop columns (make sure to assign the modified DataFrame back to self.df)\n",
        "        self.df = self.df.drop(columns=[' ANNUAL RAINFALL', 'SUBDIVISION'])\n",
        "\n",
        "\n",
        "        # Split dataframe into data and labels\n",
        "        self.data = self.df.drop(columns=['FLOODS'])\n",
        "        self.labels = self.df['FLOODS'].replace(['YES', 'NO'], [1, 0])\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.data.iloc[index]\n",
        "        label = self.labels.iloc[index]\n",
        "\n",
        "        # Convert data and label to PyTorch tensors\n",
        "        data_tensor = torch.tensor(item.values, dtype=torch.float32)\n",
        "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "        # Apply transformations if available\n",
        "        if self.transform:\n",
        "            data_tensor = self.transform(data_tensor)\n",
        "\n",
        "        return data_tensor, label_tensor\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "transform = None\n",
        "dataset = FloodDataset('/content/datasets/cleaned_data.csv', transform=transform)\n",
        "\n",
        "# Split into training and testing sets\n",
        "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create DataLoader instances\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x = flood_data.iloc[:,1:14]\n",
        "\n",
        "x"
      ],
      "metadata": {
        "id": "5JSa09f8lbnk",
        "outputId": "ab609ad8-e779-419b-f601-c24d0cbb39b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "id": "5JSa09f8lbnk",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     YEAR   JAN   FEB   MAR    APR    MAY     JUN     JUL     AUG    SEP  \\\n",
              "0    1901  28.7  44.7  51.6  160.0  174.7   824.6   743.0   357.5  197.7   \n",
              "1    1902   6.7   2.6  57.3   83.9  134.5   390.9  1205.0   315.8  491.6   \n",
              "2    1903   3.2  18.6   3.1   83.6  249.7   558.6  1022.5   420.2  341.8   \n",
              "3    1904  23.7   3.0  32.2   71.5  235.7  1098.2   725.5   351.8  222.7   \n",
              "4    1905   1.2  22.3   9.4  105.9  263.3   850.2   520.5   293.6  217.2   \n",
              "..    ...   ...   ...   ...    ...    ...     ...     ...     ...    ...   \n",
              "113  2014   4.6  10.3  17.9   95.7  251.0   454.4   677.8   733.9  298.8   \n",
              "114  2015   3.1   5.8  50.1  214.1  201.8   563.6   406.0   252.2  292.9   \n",
              "115  2016   2.4   3.8  35.9  143.0  186.4   522.2   412.3   325.5  173.2   \n",
              "116  2017   1.9   6.8   8.9   43.6  173.5   498.5   319.6   531.8  209.5   \n",
              "117  2018  29.1  52.1  48.6  116.4  183.8   625.4  1048.5  1398.9  423.6   \n",
              "\n",
              "       OCT    NOV    DEC  \n",
              "0    266.9  350.8   48.4  \n",
              "1    358.4  158.3  121.5  \n",
              "2    354.1  157.0   59.0  \n",
              "3    328.1   33.9    3.3  \n",
              "4    383.5   74.4    0.2  \n",
              "..     ...    ...    ...  \n",
              "113  355.5   99.5   47.2  \n",
              "114  308.1  223.6   79.4  \n",
              "115  225.9  125.4   23.6  \n",
              "116  192.4   92.5   38.1  \n",
              "117  356.1  125.4   65.1  \n",
              "\n",
              "[118 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eaa84ffc-4a41-4ce2-9c5b-586db21900ec\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>YEAR</th>\n",
              "      <th>JAN</th>\n",
              "      <th>FEB</th>\n",
              "      <th>MAR</th>\n",
              "      <th>APR</th>\n",
              "      <th>MAY</th>\n",
              "      <th>JUN</th>\n",
              "      <th>JUL</th>\n",
              "      <th>AUG</th>\n",
              "      <th>SEP</th>\n",
              "      <th>OCT</th>\n",
              "      <th>NOV</th>\n",
              "      <th>DEC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1901</td>\n",
              "      <td>28.7</td>\n",
              "      <td>44.7</td>\n",
              "      <td>51.6</td>\n",
              "      <td>160.0</td>\n",
              "      <td>174.7</td>\n",
              "      <td>824.6</td>\n",
              "      <td>743.0</td>\n",
              "      <td>357.5</td>\n",
              "      <td>197.7</td>\n",
              "      <td>266.9</td>\n",
              "      <td>350.8</td>\n",
              "      <td>48.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1902</td>\n",
              "      <td>6.7</td>\n",
              "      <td>2.6</td>\n",
              "      <td>57.3</td>\n",
              "      <td>83.9</td>\n",
              "      <td>134.5</td>\n",
              "      <td>390.9</td>\n",
              "      <td>1205.0</td>\n",
              "      <td>315.8</td>\n",
              "      <td>491.6</td>\n",
              "      <td>358.4</td>\n",
              "      <td>158.3</td>\n",
              "      <td>121.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1903</td>\n",
              "      <td>3.2</td>\n",
              "      <td>18.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>83.6</td>\n",
              "      <td>249.7</td>\n",
              "      <td>558.6</td>\n",
              "      <td>1022.5</td>\n",
              "      <td>420.2</td>\n",
              "      <td>341.8</td>\n",
              "      <td>354.1</td>\n",
              "      <td>157.0</td>\n",
              "      <td>59.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1904</td>\n",
              "      <td>23.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>32.2</td>\n",
              "      <td>71.5</td>\n",
              "      <td>235.7</td>\n",
              "      <td>1098.2</td>\n",
              "      <td>725.5</td>\n",
              "      <td>351.8</td>\n",
              "      <td>222.7</td>\n",
              "      <td>328.1</td>\n",
              "      <td>33.9</td>\n",
              "      <td>3.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1905</td>\n",
              "      <td>1.2</td>\n",
              "      <td>22.3</td>\n",
              "      <td>9.4</td>\n",
              "      <td>105.9</td>\n",
              "      <td>263.3</td>\n",
              "      <td>850.2</td>\n",
              "      <td>520.5</td>\n",
              "      <td>293.6</td>\n",
              "      <td>217.2</td>\n",
              "      <td>383.5</td>\n",
              "      <td>74.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>2014</td>\n",
              "      <td>4.6</td>\n",
              "      <td>10.3</td>\n",
              "      <td>17.9</td>\n",
              "      <td>95.7</td>\n",
              "      <td>251.0</td>\n",
              "      <td>454.4</td>\n",
              "      <td>677.8</td>\n",
              "      <td>733.9</td>\n",
              "      <td>298.8</td>\n",
              "      <td>355.5</td>\n",
              "      <td>99.5</td>\n",
              "      <td>47.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>2015</td>\n",
              "      <td>3.1</td>\n",
              "      <td>5.8</td>\n",
              "      <td>50.1</td>\n",
              "      <td>214.1</td>\n",
              "      <td>201.8</td>\n",
              "      <td>563.6</td>\n",
              "      <td>406.0</td>\n",
              "      <td>252.2</td>\n",
              "      <td>292.9</td>\n",
              "      <td>308.1</td>\n",
              "      <td>223.6</td>\n",
              "      <td>79.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>2016</td>\n",
              "      <td>2.4</td>\n",
              "      <td>3.8</td>\n",
              "      <td>35.9</td>\n",
              "      <td>143.0</td>\n",
              "      <td>186.4</td>\n",
              "      <td>522.2</td>\n",
              "      <td>412.3</td>\n",
              "      <td>325.5</td>\n",
              "      <td>173.2</td>\n",
              "      <td>225.9</td>\n",
              "      <td>125.4</td>\n",
              "      <td>23.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>2017</td>\n",
              "      <td>1.9</td>\n",
              "      <td>6.8</td>\n",
              "      <td>8.9</td>\n",
              "      <td>43.6</td>\n",
              "      <td>173.5</td>\n",
              "      <td>498.5</td>\n",
              "      <td>319.6</td>\n",
              "      <td>531.8</td>\n",
              "      <td>209.5</td>\n",
              "      <td>192.4</td>\n",
              "      <td>92.5</td>\n",
              "      <td>38.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>2018</td>\n",
              "      <td>29.1</td>\n",
              "      <td>52.1</td>\n",
              "      <td>48.6</td>\n",
              "      <td>116.4</td>\n",
              "      <td>183.8</td>\n",
              "      <td>625.4</td>\n",
              "      <td>1048.5</td>\n",
              "      <td>1398.9</td>\n",
              "      <td>423.6</td>\n",
              "      <td>356.1</td>\n",
              "      <td>125.4</td>\n",
              "      <td>65.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>118 rows Ã— 13 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eaa84ffc-4a41-4ce2-9c5b-586db21900ec')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-eaa84ffc-4a41-4ce2-9c5b-586db21900ec button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-eaa84ffc-4a41-4ce2-9c5b-586db21900ec');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9055c8e3-9a8d-4ace-b9c0-c6c5204406ba\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9055c8e3-9a8d-4ace-b9c0-c6c5204406ba')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9055c8e3-9a8d-4ace-b9c0-c6c5204406ba button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = flood_data.iloc[:,-1].replace(['YES', 'NO'], [1, 0])\n",
        "y"
      ],
      "metadata": {
        "id": "rKN49_bamXXR",
        "outputId": "b2096740-4eb0-4ef0-b361-86dec568d7fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "rKN49_bamXXR",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      1\n",
              "1      1\n",
              "2      1\n",
              "3      1\n",
              "4      0\n",
              "      ..\n",
              "113    1\n",
              "114    0\n",
              "115    0\n",
              "116    0\n",
              "117    1\n",
              "Name: FLOODS, Length: 118, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "6824a148",
      "metadata": {
        "id": "6824a148"
      },
      "outputs": [],
      "source": [
        "x_train, x_test = train_test_split(x, test_size=0.2, random_state=42)\n",
        "y_train, y_test = train_test_split(y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b76ea83a",
      "metadata": {
        "id": "b76ea83a"
      },
      "outputs": [],
      "source": [
        "# import torchvision.transforms as transforms\n",
        "# from torchvision import datasets\n",
        "\n",
        "# training_x = torch.tensor(x_train.values, dtype=torch.float32)\n",
        "# testing_x = torch.tensor(x_test.values, dtype=torch.float32)\n",
        "# training_y = torch.tensor(y_train.values, dtype=torch.float32)\n",
        "# testing_y = torch.tensor(y_test.values, dtype=torch.float32)\n",
        "\n",
        "# train_data =FloodDataset(training_data,transform= transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c6dd25d",
      "metadata": {
        "id": "1c6dd25d"
      },
      "outputs": [],
      "source": [
        "# from torch.utils.data.dataloader import DataLoader\n",
        "# from torch.utils import data\n",
        "# #dataset chunk to use for each iteration\n",
        "batchsize = 32\n",
        "\n",
        "# train_dataloader = DataLoader(training_x, batch_size= batchsize)\n",
        "# test_dataloader = DataLoader(testing_x, batch_size = batchsize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "d2222443",
      "metadata": {
        "id": "d2222443"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cdbb7416",
      "metadata": {
        "id": "cdbb7416"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Distra(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(Distra, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 0:\n",
        "            raise ValueError(\"Input tensor has no dimensions\")\n",
        "\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # Use the output from the last time step\n",
        "        out_last_step = out[:, -1, :]\n",
        "\n",
        "        # Adjust the Linear layer input size based on the output of the LSTM\n",
        "        out_fc = self.fc(out_last_step)\n",
        "\n",
        "        return out_fc\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f2af48ba",
      "metadata": {
        "id": "f2af48ba"
      },
      "outputs": [],
      "source": [
        "input_size = 13\n",
        "hidden_size = 64\n",
        "num_layers = 2\n",
        "output_size = 1\n",
        "model = Distra(input_size, hidden_size, num_layers, output_size)\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "9d936e21",
      "metadata": {
        "id": "9d936e21"
      },
      "outputs": [],
      "source": [
        "# the training function\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch,( X, y) in enumerate(dataloader):\n",
        "        X, y =  X.to(device).unsqueeze(0), y.to(device).unsqueeze(0)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        #Zero_grad  sens the gradient to zero after every iteration\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "1cc34235",
      "metadata": {
        "id": "1cc34235"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device).unsqueeze(0), y.to(device).unsqueeze(0)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "b9f7960c",
      "metadata": {
        "id": "b9f7960c",
        "outputId": "f7bea75d-3ed8-48d0-9a7a-a36f31e7116b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 32])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 30])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 24])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.558863  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.687935 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.509704  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.643226 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.452200  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.607885 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.495193  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.592914 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.579142  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.582750 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.397192  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.555437 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.432441  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.545367 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.392132  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.536485 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.377450  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.525554 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.427204  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.514959 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.506534  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.507327 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.326209  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.499681 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.547123  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.491986 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.421768  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.484959 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.355634  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.477925 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.454014  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.446395 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.410458  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.440550 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.437760  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.434798 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.428941  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.429423 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.359135  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.422907 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.372202  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.411341 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.363024  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.406459 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.396320  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.401802 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.378720  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.397282 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.308090  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.392831 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.364482  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.388710 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.282158  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.384653 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.343517  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.380619 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.314745  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.376656 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.310144  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.372990 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.370676  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.369039 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.311739  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.365316 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.319163  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.362078 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.396897  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.358325 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.353629  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.355070 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.329304  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.351806 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.313417  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.348804 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.292095  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.345864 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.295406  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.342865 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.347801  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.340165 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.292360  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.337261 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.302119  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.333833 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.349930  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.331138 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.425900  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.328509 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.325535  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.325885 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.277387  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.323168 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.345572  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.320691 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.281719  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.318606 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.301652  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.316561 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.286734  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.314650 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.295200  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.312841 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.332942  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.310980 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 0.279330  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.309210 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 0.248654  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.307591 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.269079  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.305911 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.257929  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.304358 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 0.333527  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.302707 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 0.255897  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.301240 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 0.247024  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.299785 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 0.235443  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.298401 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "loss: 0.264548  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.297131 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "loss: 0.264163  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.295872 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "loss: 0.253542  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.294660 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "loss: 0.263740  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.293429 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "loss: 0.261351  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.292159 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "loss: 0.225605  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.290993 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "loss: 0.234507  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.289772 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "loss: 0.252407  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.288715 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "loss: 0.242851  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.287613 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "loss: 0.266634  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.286597 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "loss: 0.272175  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.285576 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "loss: 0.298129  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.284594 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "loss: 0.260188  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.283608 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "loss: 0.307916  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.282700 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "loss: 0.225764  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.281751 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "loss: 0.280021  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.280877 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "loss: 0.257277  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.280073 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "loss: 0.227684  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.279313 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "loss: 0.242917  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.278445 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "loss: 0.247893  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.277638 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "epochs = 80\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_loader, model, loss_fn, optimizer)\n",
        "    test(test_loader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "e2f59095",
      "metadata": {
        "id": "e2f59095",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55d71024-667d-40f0-9464-43d436c57c9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved PyTorch Model State to model.pth\n"
          ]
        }
      ],
      "source": [
        "#save model\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "print(\"Saved PyTorch Model State to model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "aa6dadeb",
      "metadata": {
        "id": "aa6dadeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fa7a0e1-1cbf-4c74-b5ad-33bf385668b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "\n",
        "model.load_state_dict(torch.load(\"model.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "ba97ce89",
      "metadata": {
        "id": "ba97ce89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "420f6ebd-20c6-4da8-df33-30a543596992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56    1\n",
            "Name: FLOODS, dtype: int64\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "model.eval()\n",
        "\n",
        "x = torch.tensor(x_test.head(1).values, dtype=torch.float32).unsqueeze(0)\n",
        "y = torch.tensor(y_test.head(1).values, dtype=torch.float32).unsqueeze(0)\n",
        "print(y_test.head(1))\n",
        "\n",
        "with torch.no_grad():\n",
        "    predict = model(x)\n",
        "\n",
        "\n",
        "threshold = 0.4\n",
        "predicted_labels = (predict >= threshold).int()\n",
        "\n",
        "predicted_label = predicted_labels.item()\n",
        "\n",
        "print(predicted_label)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Load the model file\n",
        "\n",
        "checkpoint = torch.load('model.pth')\n",
        "\n",
        "# Check the contents of the checkpoint\n",
        "print(checkpoint)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk1A39NHRsKC",
        "outputId": "aa833a4f-fd72-4531-9e04-a7a0b20e2a2e"
      },
      "id": "sk1A39NHRsKC",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('lstm.weight_ih_l0', tensor([[-0.0185,  0.1083,  0.0066,  ..., -0.0925,  0.1008,  0.0097],\n",
            "        [ 0.1172, -0.1174,  0.0175,  ..., -0.0311,  0.0486,  0.0611],\n",
            "        [ 0.0050, -0.1239,  0.0707,  ...,  0.0739,  0.0268, -0.0422],\n",
            "        ...,\n",
            "        [-0.0535,  0.1187, -0.0762,  ..., -0.0562,  0.0404, -0.0475],\n",
            "        [-0.1226,  0.0685,  0.1010,  ..., -0.0937, -0.0571, -0.0424],\n",
            "        [-0.1052, -0.0638, -0.0781,  ..., -0.0879,  0.0701, -0.1246]])), ('lstm.weight_hh_l0', tensor([[ 0.0050, -0.0155, -0.1230,  ...,  0.0220, -0.1132,  0.1196],\n",
            "        [ 0.1173, -0.0155, -0.0242,  ...,  0.1177, -0.0299, -0.0244],\n",
            "        [-0.0314,  0.0327, -0.0557,  ..., -0.1142, -0.0268,  0.0127],\n",
            "        ...,\n",
            "        [ 0.0140, -0.0014,  0.0556,  ...,  0.0460, -0.0572, -0.0610],\n",
            "        [ 0.1176,  0.0490, -0.1025,  ...,  0.1218, -0.0426,  0.0141],\n",
            "        [ 0.1204, -0.0518,  0.0760,  ...,  0.0381, -0.0939,  0.0650]])), ('lstm.bias_ih_l0', tensor([ 0.0446, -0.0130,  0.0079, -0.0121,  0.0746,  0.1215,  0.0932,  0.0122,\n",
            "        -0.0202,  0.1006,  0.0148,  0.0867,  0.0117, -0.0852,  0.0549, -0.0673,\n",
            "        -0.0353,  0.0171, -0.1091, -0.1037,  0.0459,  0.0739,  0.0703,  0.0354,\n",
            "         0.0339,  0.0486, -0.0957,  0.1125, -0.0321, -0.1005, -0.1000,  0.1151,\n",
            "         0.0834,  0.0010, -0.1033, -0.0814, -0.0494, -0.1177, -0.0450,  0.0927,\n",
            "         0.0325,  0.1165,  0.0466, -0.1139, -0.0652,  0.0528, -0.0678,  0.1249,\n",
            "        -0.0174,  0.0184, -0.1176, -0.0445,  0.0412,  0.0463,  0.0157, -0.1145,\n",
            "         0.0197, -0.0649, -0.0182,  0.0645,  0.0428, -0.0488,  0.1014,  0.0574,\n",
            "         0.0612, -0.0840,  0.0924,  0.0870, -0.0438,  0.0443, -0.0293, -0.0137,\n",
            "        -0.0574, -0.1172,  0.0722,  0.0912,  0.1048,  0.0850, -0.0648, -0.0619,\n",
            "         0.1015,  0.0187,  0.0994,  0.0939, -0.1160, -0.0996, -0.0942, -0.0576,\n",
            "         0.0387, -0.0334, -0.1184,  0.0555,  0.1005, -0.0546,  0.0064, -0.0839,\n",
            "        -0.0349,  0.0645, -0.0144,  0.0938,  0.0917, -0.0034, -0.0466,  0.1161,\n",
            "         0.0716, -0.1043, -0.0239,  0.1076, -0.0852,  0.1012,  0.1078, -0.0483,\n",
            "         0.0140,  0.0002,  0.0116, -0.0020, -0.1170,  0.0749,  0.0926, -0.0973,\n",
            "         0.0176,  0.1164, -0.0616,  0.0433,  0.1210,  0.0713, -0.1052, -0.1076,\n",
            "        -0.1148,  0.0690,  0.0762,  0.0106, -0.0852,  0.1204,  0.1053,  0.0102,\n",
            "        -0.0611,  0.0754, -0.0538,  0.1082, -0.1130, -0.0169,  0.0932, -0.0627,\n",
            "        -0.0521,  0.0225,  0.0753,  0.0965,  0.0309, -0.0697,  0.0215,  0.0818,\n",
            "         0.0004,  0.0753,  0.0125, -0.0898, -0.0099,  0.0777,  0.0640, -0.0210,\n",
            "         0.0025,  0.0071,  0.0237, -0.0570,  0.0873, -0.0686, -0.0550,  0.0772,\n",
            "         0.0033, -0.0469, -0.0566,  0.0085,  0.0564,  0.0101,  0.0159, -0.0599,\n",
            "         0.0258, -0.0245, -0.1199,  0.0149, -0.0143,  0.1236,  0.0433,  0.1222,\n",
            "         0.0104,  0.1113, -0.1178, -0.0477, -0.1045,  0.0741, -0.1062, -0.1142,\n",
            "        -0.0508,  0.1148,  0.0976,  0.0620, -0.0580,  0.0874, -0.0506,  0.0487,\n",
            "        -0.0242, -0.0460,  0.1222, -0.0771, -0.0345,  0.0344, -0.0592, -0.0394,\n",
            "         0.0608, -0.0941,  0.1218, -0.0509,  0.0922, -0.0164,  0.0831, -0.0046,\n",
            "         0.0864, -0.0719, -0.1098, -0.0467,  0.0007, -0.1062,  0.0851,  0.0963,\n",
            "         0.1103,  0.0608,  0.0312, -0.0895,  0.0075,  0.0462, -0.1052,  0.1009,\n",
            "        -0.1139, -0.1167,  0.0346, -0.0444, -0.0120, -0.0714, -0.0215, -0.0546,\n",
            "         0.0807, -0.1236,  0.0739, -0.0114,  0.1232,  0.0813,  0.0933,  0.0865,\n",
            "         0.1051,  0.0910,  0.0353,  0.0073,  0.1103, -0.0690, -0.0090, -0.0626])), ('lstm.bias_hh_l0', tensor([-0.0679, -0.1211, -0.0319,  0.0533, -0.0067, -0.0367,  0.0609,  0.0476,\n",
            "         0.0939,  0.0435, -0.1219,  0.0270,  0.0487,  0.0777,  0.0737, -0.1108,\n",
            "         0.0562,  0.0657, -0.0228,  0.0077, -0.0171,  0.0264,  0.0610, -0.0442,\n",
            "        -0.0734, -0.1143,  0.0311, -0.0146, -0.0863,  0.1106,  0.1055, -0.1158,\n",
            "         0.0103,  0.0543,  0.0265, -0.0547, -0.1170,  0.0105,  0.0400,  0.0546,\n",
            "         0.0267,  0.0469,  0.0147, -0.0814, -0.0045, -0.0366, -0.0649, -0.0507,\n",
            "        -0.1172, -0.0386,  0.0223,  0.0443, -0.1100, -0.1101,  0.1106,  0.0321,\n",
            "         0.0437, -0.0427, -0.0277,  0.0092,  0.0032, -0.0560, -0.1212, -0.0636,\n",
            "         0.1122,  0.0008,  0.1075,  0.0287, -0.0707,  0.1244,  0.0238,  0.0770,\n",
            "        -0.0525,  0.0415, -0.0594,  0.1106, -0.0844,  0.0102, -0.0620,  0.1007,\n",
            "        -0.1025, -0.1019,  0.0288,  0.0291,  0.1087, -0.0932, -0.0830,  0.0517,\n",
            "         0.0569, -0.0932, -0.0428, -0.0123, -0.0168,  0.0420,  0.1241, -0.0839,\n",
            "        -0.0325, -0.1050, -0.0404,  0.0333,  0.0187,  0.0833,  0.0666, -0.0187,\n",
            "        -0.0978, -0.1078,  0.0707,  0.0445, -0.0935,  0.0364,  0.0233,  0.1199,\n",
            "         0.0613, -0.1013, -0.0456,  0.0388,  0.0125, -0.1062,  0.1132,  0.1000,\n",
            "        -0.0412, -0.1196, -0.0011,  0.0907, -0.0159,  0.0885, -0.1182, -0.1058,\n",
            "        -0.0270, -0.1108,  0.0544,  0.0509,  0.0250, -0.0501, -0.0799, -0.0494,\n",
            "         0.0547,  0.0388,  0.0209, -0.0644,  0.0152, -0.1239,  0.0498, -0.0184,\n",
            "         0.0873, -0.0819,  0.1040,  0.0042, -0.0869,  0.0190,  0.0051,  0.0497,\n",
            "        -0.0935, -0.0470, -0.0280, -0.0714,  0.0392,  0.0173, -0.0907, -0.0304,\n",
            "         0.0310, -0.0194, -0.0569, -0.0025,  0.1174, -0.0119,  0.0825, -0.0901,\n",
            "        -0.0178,  0.1136, -0.0762,  0.0601, -0.1061,  0.0455, -0.0829, -0.1169,\n",
            "        -0.0125, -0.0555,  0.0699, -0.0634, -0.0201, -0.1040, -0.0743,  0.0300,\n",
            "        -0.0966, -0.1121,  0.0925, -0.0665, -0.0227,  0.0395, -0.1200,  0.0725,\n",
            "         0.0979, -0.1013, -0.0881, -0.0382, -0.0109, -0.0892, -0.0559,  0.1182,\n",
            "        -0.0201, -0.1179, -0.1007,  0.0494,  0.0030, -0.1046, -0.0285,  0.0427,\n",
            "         0.0866,  0.0600, -0.1077, -0.0476, -0.0239, -0.0821,  0.0443, -0.0764,\n",
            "         0.0647, -0.0792, -0.0644, -0.0120, -0.0022, -0.1049, -0.1107, -0.0876,\n",
            "         0.0544, -0.0427,  0.0658, -0.0315,  0.0807, -0.0246, -0.0353, -0.0414,\n",
            "        -0.0349, -0.0404, -0.0372, -0.0783, -0.0743,  0.0247,  0.0625,  0.0197,\n",
            "        -0.0527, -0.0107, -0.0119,  0.0464, -0.0215, -0.0193, -0.0726,  0.0926,\n",
            "         0.0523, -0.0907, -0.0078, -0.0677, -0.0720,  0.0027,  0.0339, -0.0098])), ('lstm.weight_ih_l1', tensor([[-0.0544,  0.0376, -0.1235,  ..., -0.0203,  0.0923, -0.0939],\n",
            "        [ 0.0587,  0.1121, -0.1174,  ..., -0.0840,  0.1183, -0.0389],\n",
            "        [ 0.0210,  0.0141,  0.0865,  ..., -0.0827, -0.0063, -0.1206],\n",
            "        ...,\n",
            "        [-0.0839, -0.0651, -0.0388,  ..., -0.0519,  0.0368, -0.0290],\n",
            "        [-0.0615,  0.1262, -0.1174,  ..., -0.0305, -0.0072, -0.0779],\n",
            "        [ 0.0423, -0.0624,  0.0834,  ..., -0.0430, -0.0083, -0.0555]])), ('lstm.weight_hh_l1', tensor([[ 0.0937,  0.0481,  0.1007,  ...,  0.1142,  0.0670,  0.1004],\n",
            "        [ 0.0798,  0.0984, -0.0003,  ..., -0.1146, -0.1191,  0.0365],\n",
            "        [ 0.0305, -0.0680,  0.1114,  ...,  0.1056,  0.0338, -0.0721],\n",
            "        ...,\n",
            "        [-0.0275,  0.0023, -0.0088,  ...,  0.0778,  0.0508, -0.0313],\n",
            "        [ 0.0273,  0.0393,  0.0162,  ...,  0.0711,  0.0383,  0.0500],\n",
            "        [-0.0977, -0.1217, -0.0295,  ...,  0.0362,  0.0296,  0.0902]])), ('lstm.bias_ih_l1', tensor([ 0.0854,  0.0708,  0.1215,  0.0651, -0.0562, -0.0110,  0.0891,  0.0868,\n",
            "        -0.0861, -0.1180, -0.1181,  0.1179,  0.1203,  0.0535,  0.0601,  0.1008,\n",
            "         0.0679,  0.0093, -0.0516,  0.0873, -0.0290, -0.1218,  0.0083, -0.0837,\n",
            "         0.0647,  0.0280,  0.1223, -0.0558,  0.0284,  0.0454, -0.1160,  0.1033,\n",
            "         0.0819,  0.0185,  0.1209,  0.0741,  0.0503,  0.0151,  0.0682,  0.1098,\n",
            "        -0.0696, -0.0339,  0.0905, -0.0203, -0.0018, -0.1124, -0.0636,  0.0813,\n",
            "        -0.0996,  0.0917,  0.0377,  0.0388,  0.0319, -0.0571, -0.1155, -0.1092,\n",
            "        -0.0683,  0.1111, -0.0349,  0.1069,  0.0853, -0.0164,  0.1015,  0.1078,\n",
            "        -0.1133, -0.1234,  0.0123, -0.0566, -0.0801, -0.0262, -0.0089,  0.1057,\n",
            "         0.1172,  0.0705,  0.0423,  0.1151,  0.0890, -0.0308,  0.0889,  0.0363,\n",
            "        -0.0780, -0.0407, -0.0559,  0.0255,  0.0339,  0.0596, -0.0081,  0.1033,\n",
            "        -0.0243,  0.0131,  0.0679, -0.1008,  0.0910, -0.0186,  0.0429,  0.0524,\n",
            "         0.0306, -0.0108,  0.1145,  0.0258, -0.0969, -0.0431,  0.0486,  0.0617,\n",
            "        -0.0282, -0.0359, -0.0959,  0.0161, -0.0137,  0.0665,  0.0669, -0.0513,\n",
            "        -0.0090, -0.0060, -0.0436, -0.0538,  0.0591, -0.0072,  0.0347, -0.0927,\n",
            "        -0.0342,  0.0131, -0.0823,  0.0405, -0.0682, -0.1161,  0.1008,  0.1172,\n",
            "         0.0744,  0.1134, -0.0396,  0.1193, -0.1065,  0.1009,  0.0417,  0.0063,\n",
            "        -0.0458, -0.0791, -0.0536, -0.1042,  0.0607,  0.1133,  0.0620,  0.0099,\n",
            "        -0.0262,  0.0998, -0.0847, -0.0055, -0.0779, -0.0653, -0.0515,  0.0789,\n",
            "         0.0107,  0.0963, -0.0284,  0.0263,  0.0955,  0.1200, -0.0019, -0.0283,\n",
            "        -0.0490,  0.0145, -0.0923,  0.0573, -0.0661,  0.1003, -0.0081,  0.0825,\n",
            "        -0.1106,  0.0759,  0.0682, -0.0062,  0.0704, -0.0421, -0.1194, -0.0423,\n",
            "         0.0979, -0.0706, -0.0383,  0.0137,  0.0341, -0.0636,  0.0665,  0.0350,\n",
            "         0.0091, -0.0247,  0.0509, -0.0714,  0.1061,  0.0238,  0.0195, -0.0400,\n",
            "         0.1158,  0.1179, -0.0822,  0.1058,  0.1193, -0.1067,  0.0066, -0.0399,\n",
            "        -0.0669,  0.1210, -0.0434,  0.0563,  0.0937, -0.0920, -0.0646, -0.0508,\n",
            "        -0.0949, -0.0427, -0.0973, -0.0384,  0.0039, -0.0459, -0.0885, -0.0926,\n",
            "         0.0157, -0.1062, -0.0160, -0.0677,  0.1245, -0.0900,  0.1180, -0.0042,\n",
            "         0.0098,  0.1123,  0.1117, -0.1005,  0.0548, -0.0502,  0.1121,  0.0348,\n",
            "         0.0900,  0.0532, -0.0897,  0.0941, -0.1179,  0.0031, -0.0703,  0.1242,\n",
            "        -0.0133, -0.0108, -0.1001, -0.0027, -0.0879,  0.0597,  0.0070, -0.0163,\n",
            "         0.0657,  0.0100,  0.0458,  0.0872,  0.1194, -0.0666,  0.0767, -0.0164])), ('lstm.bias_hh_l1', tensor([-0.0324,  0.1098, -0.0719, -0.0629,  0.1003,  0.0301,  0.0698,  0.0589,\n",
            "        -0.0684,  0.0640,  0.0455, -0.1191, -0.1029, -0.0521, -0.0180, -0.0554,\n",
            "        -0.0266, -0.1150, -0.0625, -0.0108, -0.0158,  0.0133, -0.0503,  0.0529,\n",
            "         0.0086, -0.0268, -0.0416, -0.0629, -0.0205,  0.0982,  0.1107, -0.0869,\n",
            "        -0.1029, -0.0917, -0.1119,  0.0054, -0.0609,  0.0054,  0.0707,  0.0108,\n",
            "        -0.0649,  0.1019, -0.0397,  0.0446,  0.0340,  0.0393,  0.0394,  0.1155,\n",
            "         0.0356, -0.0377, -0.0564, -0.0112, -0.0550, -0.0069, -0.0982, -0.0568,\n",
            "        -0.1176, -0.0868, -0.0537, -0.1228, -0.0945,  0.0973, -0.1218,  0.0341,\n",
            "        -0.0992, -0.1241, -0.0752,  0.0385, -0.0328, -0.0081,  0.1187,  0.0129,\n",
            "        -0.0790,  0.0188,  0.1017,  0.0617,  0.0886, -0.1221, -0.1077, -0.1184,\n",
            "        -0.0220, -0.0027,  0.0800, -0.1124,  0.0954,  0.0220, -0.1032,  0.0075,\n",
            "         0.1085, -0.0640,  0.1235,  0.0648, -0.0021,  0.1142,  0.0367, -0.0200,\n",
            "         0.1079,  0.0710, -0.0947, -0.1005, -0.0238,  0.0647,  0.1066,  0.0950,\n",
            "         0.1143,  0.1028, -0.0914,  0.1173, -0.1098,  0.0298,  0.1021,  0.0960,\n",
            "         0.0848, -0.0814, -0.1111, -0.0293, -0.0946, -0.0496, -0.0713, -0.0998,\n",
            "        -0.0224,  0.0604,  0.1130,  0.0084, -0.0880,  0.0935,  0.0521, -0.1016,\n",
            "        -0.0309,  0.1138,  0.0974,  0.0357, -0.0287, -0.0163,  0.1063,  0.0872,\n",
            "         0.0430, -0.0636,  0.1069, -0.0181,  0.1130,  0.0541, -0.0724, -0.0006,\n",
            "        -0.0554,  0.1049,  0.0808, -0.0163,  0.0570, -0.0741, -0.0873, -0.0561,\n",
            "         0.0552, -0.0062, -0.0625,  0.0805, -0.0215,  0.0335, -0.0056, -0.0439,\n",
            "        -0.1203,  0.0479, -0.0975,  0.0104,  0.0687, -0.0809, -0.0488,  0.0897,\n",
            "        -0.0441,  0.1179,  0.0658, -0.0839,  0.0526, -0.0337,  0.0789,  0.0613,\n",
            "         0.0604, -0.0262, -0.0634,  0.0741,  0.0441,  0.0192, -0.0292, -0.0370,\n",
            "         0.0701,  0.1130, -0.0285,  0.0544, -0.0172, -0.0431, -0.0303,  0.0402,\n",
            "         0.0427,  0.0223, -0.0115,  0.0396, -0.0046,  0.0321, -0.0105,  0.0271,\n",
            "         0.0369,  0.0599, -0.0729,  0.0697, -0.0019, -0.0388,  0.0336, -0.0785,\n",
            "         0.0223,  0.0966,  0.0504,  0.0765, -0.0987,  0.0443,  0.1248,  0.1032,\n",
            "         0.0700,  0.1004,  0.0126, -0.0400,  0.0439,  0.0895,  0.0622,  0.0110,\n",
            "         0.0289,  0.0499, -0.0296, -0.0142, -0.0347,  0.0194, -0.0402,  0.0708,\n",
            "        -0.0793, -0.0033,  0.0802,  0.0109,  0.0525, -0.1118,  0.0788,  0.1008,\n",
            "         0.1124,  0.0463, -0.0804,  0.1017, -0.0940,  0.1103, -0.1176, -0.0334,\n",
            "         0.1001,  0.0186,  0.1026, -0.0490, -0.0360, -0.0905,  0.1161, -0.0113])), ('fc.weight', tensor([[ 0.1343, -0.0464, -0.0432,  0.0097,  0.0153, -0.1160, -0.0890, -0.0033,\n",
            "          0.0622, -0.1333,  0.0902,  0.0164, -0.0596,  0.0403, -0.1066,  0.0666,\n",
            "         -0.0154, -0.1071, -0.0514, -0.0779, -0.1163, -0.0221, -0.0551,  0.1333,\n",
            "          0.0767, -0.0063,  0.1061, -0.1133,  0.0628, -0.0004, -0.0873,  0.0275,\n",
            "         -0.0118,  0.0550,  0.0700, -0.0752,  0.0783,  0.0434, -0.1005, -0.0561,\n",
            "         -0.0141, -0.0524, -0.0207, -0.0121, -0.1081, -0.0373, -0.0390,  0.0298,\n",
            "          0.0388,  0.0607,  0.0651,  0.1158, -0.0808,  0.0343,  0.0653,  0.1348,\n",
            "          0.0173,  0.1493, -0.0460, -0.0365, -0.1068, -0.1094,  0.1439,  0.1109]])), ('fc.bias', tensor([0.0996]))])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}