{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8190b79",
      "metadata": {
        "id": "b8190b79"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "dca9bee4",
      "metadata": {
        "id": "dca9bee4"
      },
      "outputs": [],
      "source": [
        "flood_data = pd.read_csv(\"/content/datasets/kerala.csv\")\n",
        "df_cleaned = flood_data.dropna()\n",
        "\n",
        "# Save the cleaned DataFrame to a new CSV file\n",
        "df_cleaned.to_csv('/content/datasets/cleaned_data.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccdc43ac",
      "metadata": {
        "id": "ccdc43ac",
        "outputId": "6dc1c487-db8a-4a4f-c10e-52feb03fd939",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.40)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.35.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wandb\n",
        "\n",
        "# Paste your api key here\n",
        "os.environ[\"WANDB_API_KEY\"] = '5113c6e4fced8cbf6ed17e7bfd6402b76f3fc454'\n",
        "\n",
        "wandb.init()\n",
        "# Feel free to change these and experiment !!\n",
        "config = wandb.config\n",
        "config.learning_rate = 2e-5\n",
        "config.batch_size = 32\n",
        "config.output_size = 1\n",
        "config.input_size = 1\n",
        "config.hidden_size = 64\n",
        "config.epochs = 80"
      ],
      "metadata": {
        "id": "kLx5MvgeCdv8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "087ffae1-9862-4381-de50-be5006589761"
      },
      "id": "kLx5MvgeCdv8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfmbishu\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231116_083854-klpu8lux</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/fmbishu/uncategorized/runs/klpu8lux' target=\"_blank\">easy-glitter-1</a></strong> to <a href='https://wandb.ai/fmbishu/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/fmbishu/uncategorized' target=\"_blank\">https://wandb.ai/fmbishu/uncategorized</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/fmbishu/uncategorized/runs/klpu8lux' target=\"_blank\">https://wandb.ai/fmbishu/uncategorized/runs/klpu8lux</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4c0a2ece",
      "metadata": {
        "id": "4c0a2ece"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class FloodDataset(Dataset):\n",
        "    def __init__(self, data_file, transform=None):\n",
        "        self.df = pd.read_csv(data_file)\n",
        "\n",
        "        # Drop columns (make sure to assign the modified DataFrame back to self.df)\n",
        "        self.df = self.df.drop(columns=[' ANNUAL RAINFALL', 'SUBDIVISION'])\n",
        "\n",
        "\n",
        "        # Split dataframe into data and labels\n",
        "        self.data = self.df.drop(columns=['FLOODS'])\n",
        "        self.labels = self.df['FLOODS'].replace(['YES', 'NO'], [1, 0])\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.data.iloc[index]\n",
        "        label = self.labels.iloc[index]\n",
        "\n",
        "        # Convert data and label to PyTorch tensors\n",
        "        data_tensor = torch.tensor(item.values, dtype=torch.float32)\n",
        "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "        # Apply transformations if available\n",
        "        if self.transform:\n",
        "            data_tensor = self.transform(data_tensor)\n",
        "\n",
        "        return data_tensor, label_tensor\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "transform = None\n",
        "dataset = FloodDataset('/content/datasets/cleaned_data.csv', transform=transform)\n",
        "\n",
        "# Split into training and testing sets\n",
        "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create DataLoader instances\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x = flood_data.iloc[:,1:14]\n",
        "\n",
        "x"
      ],
      "metadata": {
        "id": "5JSa09f8lbnk",
        "outputId": "ab609ad8-e779-419b-f601-c24d0cbb39b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "id": "5JSa09f8lbnk",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     YEAR   JAN   FEB   MAR    APR    MAY     JUN     JUL     AUG    SEP  \\\n",
              "0    1901  28.7  44.7  51.6  160.0  174.7   824.6   743.0   357.5  197.7   \n",
              "1    1902   6.7   2.6  57.3   83.9  134.5   390.9  1205.0   315.8  491.6   \n",
              "2    1903   3.2  18.6   3.1   83.6  249.7   558.6  1022.5   420.2  341.8   \n",
              "3    1904  23.7   3.0  32.2   71.5  235.7  1098.2   725.5   351.8  222.7   \n",
              "4    1905   1.2  22.3   9.4  105.9  263.3   850.2   520.5   293.6  217.2   \n",
              "..    ...   ...   ...   ...    ...    ...     ...     ...     ...    ...   \n",
              "113  2014   4.6  10.3  17.9   95.7  251.0   454.4   677.8   733.9  298.8   \n",
              "114  2015   3.1   5.8  50.1  214.1  201.8   563.6   406.0   252.2  292.9   \n",
              "115  2016   2.4   3.8  35.9  143.0  186.4   522.2   412.3   325.5  173.2   \n",
              "116  2017   1.9   6.8   8.9   43.6  173.5   498.5   319.6   531.8  209.5   \n",
              "117  2018  29.1  52.1  48.6  116.4  183.8   625.4  1048.5  1398.9  423.6   \n",
              "\n",
              "       OCT    NOV    DEC  \n",
              "0    266.9  350.8   48.4  \n",
              "1    358.4  158.3  121.5  \n",
              "2    354.1  157.0   59.0  \n",
              "3    328.1   33.9    3.3  \n",
              "4    383.5   74.4    0.2  \n",
              "..     ...    ...    ...  \n",
              "113  355.5   99.5   47.2  \n",
              "114  308.1  223.6   79.4  \n",
              "115  225.9  125.4   23.6  \n",
              "116  192.4   92.5   38.1  \n",
              "117  356.1  125.4   65.1  \n",
              "\n",
              "[118 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eaa84ffc-4a41-4ce2-9c5b-586db21900ec\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>YEAR</th>\n",
              "      <th>JAN</th>\n",
              "      <th>FEB</th>\n",
              "      <th>MAR</th>\n",
              "      <th>APR</th>\n",
              "      <th>MAY</th>\n",
              "      <th>JUN</th>\n",
              "      <th>JUL</th>\n",
              "      <th>AUG</th>\n",
              "      <th>SEP</th>\n",
              "      <th>OCT</th>\n",
              "      <th>NOV</th>\n",
              "      <th>DEC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1901</td>\n",
              "      <td>28.7</td>\n",
              "      <td>44.7</td>\n",
              "      <td>51.6</td>\n",
              "      <td>160.0</td>\n",
              "      <td>174.7</td>\n",
              "      <td>824.6</td>\n",
              "      <td>743.0</td>\n",
              "      <td>357.5</td>\n",
              "      <td>197.7</td>\n",
              "      <td>266.9</td>\n",
              "      <td>350.8</td>\n",
              "      <td>48.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1902</td>\n",
              "      <td>6.7</td>\n",
              "      <td>2.6</td>\n",
              "      <td>57.3</td>\n",
              "      <td>83.9</td>\n",
              "      <td>134.5</td>\n",
              "      <td>390.9</td>\n",
              "      <td>1205.0</td>\n",
              "      <td>315.8</td>\n",
              "      <td>491.6</td>\n",
              "      <td>358.4</td>\n",
              "      <td>158.3</td>\n",
              "      <td>121.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1903</td>\n",
              "      <td>3.2</td>\n",
              "      <td>18.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>83.6</td>\n",
              "      <td>249.7</td>\n",
              "      <td>558.6</td>\n",
              "      <td>1022.5</td>\n",
              "      <td>420.2</td>\n",
              "      <td>341.8</td>\n",
              "      <td>354.1</td>\n",
              "      <td>157.0</td>\n",
              "      <td>59.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1904</td>\n",
              "      <td>23.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>32.2</td>\n",
              "      <td>71.5</td>\n",
              "      <td>235.7</td>\n",
              "      <td>1098.2</td>\n",
              "      <td>725.5</td>\n",
              "      <td>351.8</td>\n",
              "      <td>222.7</td>\n",
              "      <td>328.1</td>\n",
              "      <td>33.9</td>\n",
              "      <td>3.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1905</td>\n",
              "      <td>1.2</td>\n",
              "      <td>22.3</td>\n",
              "      <td>9.4</td>\n",
              "      <td>105.9</td>\n",
              "      <td>263.3</td>\n",
              "      <td>850.2</td>\n",
              "      <td>520.5</td>\n",
              "      <td>293.6</td>\n",
              "      <td>217.2</td>\n",
              "      <td>383.5</td>\n",
              "      <td>74.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>2014</td>\n",
              "      <td>4.6</td>\n",
              "      <td>10.3</td>\n",
              "      <td>17.9</td>\n",
              "      <td>95.7</td>\n",
              "      <td>251.0</td>\n",
              "      <td>454.4</td>\n",
              "      <td>677.8</td>\n",
              "      <td>733.9</td>\n",
              "      <td>298.8</td>\n",
              "      <td>355.5</td>\n",
              "      <td>99.5</td>\n",
              "      <td>47.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>2015</td>\n",
              "      <td>3.1</td>\n",
              "      <td>5.8</td>\n",
              "      <td>50.1</td>\n",
              "      <td>214.1</td>\n",
              "      <td>201.8</td>\n",
              "      <td>563.6</td>\n",
              "      <td>406.0</td>\n",
              "      <td>252.2</td>\n",
              "      <td>292.9</td>\n",
              "      <td>308.1</td>\n",
              "      <td>223.6</td>\n",
              "      <td>79.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>2016</td>\n",
              "      <td>2.4</td>\n",
              "      <td>3.8</td>\n",
              "      <td>35.9</td>\n",
              "      <td>143.0</td>\n",
              "      <td>186.4</td>\n",
              "      <td>522.2</td>\n",
              "      <td>412.3</td>\n",
              "      <td>325.5</td>\n",
              "      <td>173.2</td>\n",
              "      <td>225.9</td>\n",
              "      <td>125.4</td>\n",
              "      <td>23.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>2017</td>\n",
              "      <td>1.9</td>\n",
              "      <td>6.8</td>\n",
              "      <td>8.9</td>\n",
              "      <td>43.6</td>\n",
              "      <td>173.5</td>\n",
              "      <td>498.5</td>\n",
              "      <td>319.6</td>\n",
              "      <td>531.8</td>\n",
              "      <td>209.5</td>\n",
              "      <td>192.4</td>\n",
              "      <td>92.5</td>\n",
              "      <td>38.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>2018</td>\n",
              "      <td>29.1</td>\n",
              "      <td>52.1</td>\n",
              "      <td>48.6</td>\n",
              "      <td>116.4</td>\n",
              "      <td>183.8</td>\n",
              "      <td>625.4</td>\n",
              "      <td>1048.5</td>\n",
              "      <td>1398.9</td>\n",
              "      <td>423.6</td>\n",
              "      <td>356.1</td>\n",
              "      <td>125.4</td>\n",
              "      <td>65.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>118 rows Ã— 13 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eaa84ffc-4a41-4ce2-9c5b-586db21900ec')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-eaa84ffc-4a41-4ce2-9c5b-586db21900ec button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-eaa84ffc-4a41-4ce2-9c5b-586db21900ec');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9055c8e3-9a8d-4ace-b9c0-c6c5204406ba\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9055c8e3-9a8d-4ace-b9c0-c6c5204406ba')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9055c8e3-9a8d-4ace-b9c0-c6c5204406ba button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = flood_data.iloc[:,-1].replace(['YES', 'NO'], [1, 0])\n",
        "y"
      ],
      "metadata": {
        "id": "rKN49_bamXXR",
        "outputId": "b2096740-4eb0-4ef0-b361-86dec568d7fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "rKN49_bamXXR",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      1\n",
              "1      1\n",
              "2      1\n",
              "3      1\n",
              "4      0\n",
              "      ..\n",
              "113    1\n",
              "114    0\n",
              "115    0\n",
              "116    0\n",
              "117    1\n",
              "Name: FLOODS, Length: 118, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "6824a148",
      "metadata": {
        "id": "6824a148"
      },
      "outputs": [],
      "source": [
        "x_train, x_test = train_test_split(x, test_size=0.2, random_state=42)\n",
        "y_train, y_test = train_test_split(y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b76ea83a",
      "metadata": {
        "id": "b76ea83a"
      },
      "outputs": [],
      "source": [
        "# import torchvision.transforms as transforms\n",
        "# from torchvision import datasets\n",
        "\n",
        "# training_x = torch.tensor(x_train.values, dtype=torch.float32)\n",
        "# testing_x = torch.tensor(x_test.values, dtype=torch.float32)\n",
        "# training_y = torch.tensor(y_train.values, dtype=torch.float32)\n",
        "# testing_y = torch.tensor(y_test.values, dtype=torch.float32)\n",
        "\n",
        "# train_data =FloodDataset(training_data,transform= transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c6dd25d",
      "metadata": {
        "id": "1c6dd25d"
      },
      "outputs": [],
      "source": [
        "# from torch.utils.data.dataloader import DataLoader\n",
        "# from torch.utils import data\n",
        "# #dataset chunk to use for each iteration\n",
        "batchsize = 32\n",
        "\n",
        "# train_dataloader = DataLoader(training_x, batch_size= batchsize)\n",
        "# test_dataloader = DataLoader(testing_x, batch_size = batchsize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "d2222443",
      "metadata": {
        "id": "d2222443"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cdbb7416",
      "metadata": {
        "id": "cdbb7416"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Distra(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(Distra, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 0:\n",
        "            raise ValueError(\"Input tensor has no dimensions\")\n",
        "\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # Use the output from the last time step\n",
        "        out_last_step = out[:, -1, :]\n",
        "\n",
        "        # Adjust the Linear layer input size based on the output of the LSTM\n",
        "        out_fc = self.fc(out_last_step)\n",
        "\n",
        "        return out_fc\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f2af48ba",
      "metadata": {
        "id": "f2af48ba"
      },
      "outputs": [],
      "source": [
        "input_size = 13\n",
        "hidden_size = 64\n",
        "num_layers = 2\n",
        "output_size = 1\n",
        "model = Distra(input_size, hidden_size, num_layers, output_size)\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "9d936e21",
      "metadata": {
        "id": "9d936e21"
      },
      "outputs": [],
      "source": [
        "# the training function\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch,( X, y) in enumerate(dataloader):\n",
        "        X, y =  X.to(device).unsqueeze(0), y.to(device).unsqueeze(0)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        #Zero_grad  sens the gradient to zero after every iteration\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "1cc34235",
      "metadata": {
        "id": "1cc34235"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device).unsqueeze(0), y.to(device).unsqueeze(0)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "b9f7960c",
      "metadata": {
        "id": "b9f7960c",
        "outputId": "f7bea75d-3ed8-48d0-9a7a-a36f31e7116b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 32])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 30])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 24])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.558863  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.687935 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.509704  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.643226 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.452200  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.607885 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.495193  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.592914 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.579142  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.582750 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.397192  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.555437 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.432441  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.545367 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.392132  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.536485 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.377450  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.525554 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.427204  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.514959 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.506534  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.507327 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.326209  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.499681 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.547123  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.491986 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.421768  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.484959 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.355634  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.477925 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.454014  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.446395 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.410458  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.440550 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.437760  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.434798 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.428941  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.429423 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.359135  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.422907 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.372202  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.411341 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.363024  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.406459 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.396320  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.401802 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.378720  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.397282 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.308090  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.392831 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.364482  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.388710 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.282158  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.384653 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.343517  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.380619 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.314745  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.376656 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.310144  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.372990 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.370676  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.369039 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.311739  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.365316 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.319163  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.362078 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.396897  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.358325 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.353629  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.355070 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.329304  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.351806 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.313417  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.348804 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.292095  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.345864 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.295406  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.342865 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.347801  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.340165 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.292360  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.337261 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.302119  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.333833 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.349930  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.331138 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.425900  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.328509 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.325535  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.325885 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.277387  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.323168 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.345572  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.320691 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.281719  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.318606 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.301652  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.316561 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.286734  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.314650 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.295200  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.312841 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.332942  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.310980 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 0.279330  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.309210 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 0.248654  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.307591 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.269079  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.305911 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.257929  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.304358 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 0.333527  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.302707 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 0.255897  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.301240 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 0.247024  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.299785 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 0.235443  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.298401 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "loss: 0.264548  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.297131 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "loss: 0.264163  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.295872 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "loss: 0.253542  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.294660 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "loss: 0.263740  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.293429 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "loss: 0.261351  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.292159 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "loss: 0.225605  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.290993 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "loss: 0.234507  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.289772 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "loss: 0.252407  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.288715 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "loss: 0.242851  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.287613 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "loss: 0.266634  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.286597 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "loss: 0.272175  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.285576 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "loss: 0.298129  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.284594 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "loss: 0.260188  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.283608 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "loss: 0.307916  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.282700 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "loss: 0.225764  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.281751 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "loss: 0.280021  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.280877 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "loss: 0.257277  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.280073 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "loss: 0.227684  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.279313 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "loss: 0.242917  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.278445 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "loss: 0.247893  [    1/   94]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 0.277638 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "epochs = 80\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_loader, model, loss_fn, optimizer)\n",
        "    test(test_loader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "e2f59095",
      "metadata": {
        "id": "e2f59095",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55d71024-667d-40f0-9464-43d436c57c9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved PyTorch Model State to model.pth\n"
          ]
        }
      ],
      "source": [
        "#save model\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "print(\"Saved PyTorch Model State to model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "aa6dadeb",
      "metadata": {
        "id": "aa6dadeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fa7a0e1-1cbf-4c74-b5ad-33bf385668b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "\n",
        "model.load_state_dict(torch.load(\"model.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "ba97ce89",
      "metadata": {
        "id": "ba97ce89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "420f6ebd-20c6-4da8-df33-30a543596992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56    1\n",
            "Name: FLOODS, dtype: int64\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "model.eval()\n",
        "\n",
        "x = torch.tensor(x_test.head(1).values, dtype=torch.float32).unsqueeze(0)\n",
        "y = torch.tensor(y_test.head(1).values, dtype=torch.float32).unsqueeze(0)\n",
        "print(y_test.head(1))\n",
        "\n",
        "with torch.no_grad():\n",
        "    predict = model(x)\n",
        "\n",
        "\n",
        "threshold = 0.4\n",
        "predicted_labels = (predict >= threshold).int()\n",
        "\n",
        "predicted_label = predicted_labels.item()\n",
        "\n",
        "print(predicted_label)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}